{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtlNLPpXy/vQCHS51sW8kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anpham1331/ECGR4105-Intro-to-ML/blob/main/ECGR_4105_Homework_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PRg3s3LgoLG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c357b5c-b0ee-432f-f6b0-190517aaef61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/anpham1331/ECGR4105-Intro-to-ML/tree/main\n",
        "\n",
        "#uses Housing.csv for this assignment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_file2 = \"/content/drive/MyDrive/ECGR 4105 - Intro to ML/Datasets/Housing.csv\"\n",
        "housing = pd.read_csv(path_file2)\n",
        "\n",
        "# Defining the map function for housing. Maps yes to 1 and no to 0\n",
        "varlist = ['mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea']\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0})\n",
        "# Applying the function to the housing list\n",
        "housing[varlist] = housing[varlist].apply(binary_map)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1 - a\n",
        "#input variables = all input features\n",
        "\n",
        "# Get Values\n",
        "X = housing.iloc[:, 1:11].values\n",
        "y = housing.iloc[:, 0].values\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Scaling\n",
        "sc = StandardScaler()  # Create a scaler object\n",
        "X_train = sc.fit_transform(X_train)  # Fit the scaler to the training data and transform\n",
        "X_test = sc.transform(X_test)  # Apply the scaler to the test data\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "X_val = torch.FloatTensor(X_test)\n",
        "y_val = torch.FloatTensor(y_test)\n",
        "\n",
        "# Define the Neural Network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_size = X_train.shape[1]\n",
        "        self.hidden_size = 32\n",
        "        self.output_size = 1\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression problems\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#Model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    # Calculate average training loss\n",
        "    average_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validate the model every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss}, Validation Loss: {val_loss.item()}')\n"
      ],
      "metadata": {
        "id": "jzA2NnE-HeTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cea7ce-6ac6-422d-b238-0b87f2ff3730"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Training Loss: 26394105554066.285, Validation Loss: 25185897938944.0\n",
            "Epoch 200/1000, Training Loss: 26437627749522.285, Validation Loss: 25176899059712.0\n",
            "Epoch 300/1000, Training Loss: 26321392350939.43, Validation Loss: 25163871551488.0\n",
            "Epoch 400/1000, Training Loss: 26522987528192.0, Validation Loss: 25147333410816.0\n",
            "Epoch 500/1000, Training Loss: 26263721869312.0, Validation Loss: 25127460798464.0\n",
            "Epoch 600/1000, Training Loss: 26355377747675.43, Validation Loss: 25104406806528.0\n",
            "Epoch 700/1000, Training Loss: 26583652443282.285, Validation Loss: 25078200795136.0\n",
            "Epoch 800/1000, Training Loss: 26417640992182.855, Validation Loss: 25048867930112.0\n",
            "Epoch 900/1000, Training Loss: 26583035581001.145, Validation Loss: 25016431280128.0\n",
            "Epoch 1000/1000, Training Loss: 26109407882678.855, Validation Loss: 24980798570496.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Problem 1 - b\n",
        "#input variables = all input features\n",
        "\n",
        "# Get Values\n",
        "X = housing.iloc[:, 1:11].values\n",
        "y = housing.iloc[:, 0].values\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Scaling\n",
        "sc = StandardScaler()  # Create a scaler object\n",
        "X_train = sc.fit_transform(X_train)  # Fit the scaler to the training data and transform\n",
        "X_test = sc.transform(X_test)  # Apply the scaler to the test data\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "X_val = torch.FloatTensor(X_test)\n",
        "y_val = torch.FloatTensor(y_test)\n",
        "\n",
        "# Define the Neural Network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_size = X_train.shape[1]\n",
        "        self.hidden_size1 = 32\n",
        "        self.hidden_size2 = 64\n",
        "        self.hidden_size3 = 16\n",
        "        self.output_size = 1\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(self.hidden_size2, self.hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(self.hidden_size3, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression problems\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#Model\n",
        "train_loss_values = []\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    average_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validate the model every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss}, Validation Loss: {val_loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ak2jmWTGjP",
        "outputId": "d4adfd01-47a0-44fa-ab6a-1aca85eaf548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Training Loss: 9766406979584.0, Validation Loss: 9588175798272.0\n",
            "Epoch 200/1000, Training Loss: 2855308792393.143, Validation Loss: 2887528218624.0\n",
            "Epoch 300/1000, Training Loss: 1529168297984.0, Validation Loss: 1393320656896.0\n",
            "Epoch 400/1000, Training Loss: 1330609178916.5715, Validation Loss: 1204000391168.0\n",
            "Epoch 500/1000, Training Loss: 1226053713920.0, Validation Loss: 1124214112256.0\n",
            "Epoch 600/1000, Training Loss: 1189496918601.1428, Validation Loss: 1099398971392.0\n",
            "Epoch 700/1000, Training Loss: 1168788811190.8572, Validation Loss: 1081899417600.0\n",
            "Epoch 800/1000, Training Loss: 1185441068763.4285, Validation Loss: 1075194363904.0\n",
            "Epoch 900/1000, Training Loss: 1145984979529.1428, Validation Loss: 1072084549632.0\n",
            "Epoch 1000/1000, Training Loss: 1136873139638.8572, Validation Loss: 1067640029184.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2 - a\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n",
        "\n",
        "# Define the neural network model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(32, 32, 3)))  # Flatten the input images\n",
        "model.add(layers.Dense(512, activation='relu'))  # Hidden layer with 512 neurons and ReLU activation\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = model.evaluate(x_test, y_test)\n",
        "training_loss = history.history['loss'][-1]\n",
        "evaluation_accuracy = eval_result[1]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Training Loss: {training_loss:.4f}\")\n",
        "print(f\"Evaluation Accuracy: {evaluation_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSuM033bPU-g",
        "outputId": "aa6124bf-fdbf-4955-cb75-8d8e9e2be96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 44s 27ms/step - loss: 1.8909 - accuracy: 0.3298 - val_loss: 1.7388 - val_accuracy: 0.3811\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.7044 - accuracy: 0.3891 - val_loss: 1.6268 - val_accuracy: 0.4155\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.6307 - accuracy: 0.4173 - val_loss: 1.5968 - val_accuracy: 0.4299\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5872 - accuracy: 0.4324 - val_loss: 1.5996 - val_accuracy: 0.4252\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.5562 - accuracy: 0.4442 - val_loss: 1.5503 - val_accuracy: 0.4464\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5369 - accuracy: 0.4526 - val_loss: 1.5835 - val_accuracy: 0.4358\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5216 - accuracy: 0.4573 - val_loss: 1.5734 - val_accuracy: 0.4406\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5040 - accuracy: 0.4626 - val_loss: 1.5317 - val_accuracy: 0.4462\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.4877 - accuracy: 0.4715 - val_loss: 1.5294 - val_accuracy: 0.4566\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.4761 - accuracy: 0.4743 - val_loss: 1.5613 - val_accuracy: 0.4418\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.5613 - accuracy: 0.4418\n",
            "Training Time: 384.16 seconds\n",
            "Training Loss: 1.4761\n",
            "Evaluation Accuracy: 0.4418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2 - b\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n",
        "\n",
        "# Define the extended neural network model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(32, 32, 3)))  # Flatten the input images\n",
        "model.add(layers.Dense(512, activation='relu'))  # Hidden layer 1 with 512 neurons and ReLU activation\n",
        "model.add(layers.Dense(256, activation='relu'))  # Hidden layer 2 with 256 neurons and ReLU activation\n",
        "model.add(layers.Dense(128, activation='relu'))  # Hidden layer 3 with 128 neurons and ReLU activation\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=300, validation_data=(x_test, y_test))\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = model.evaluate(x_test, y_test)\n",
        "training_loss = history.history['loss'][-1]\n",
        "evaluation_accuracy = eval_result[1]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Training Loss: {training_loss:.4f}\")\n",
        "print(f\"Evaluation Accuracy: {evaluation_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "_wwzOrXQRloy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c155069a-1e49-4761-ee0f-5d9c29009cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 1.8754 - accuracy: 0.3201 - val_loss: 1.7020 - val_accuracy: 0.3859\n",
            "Epoch 2/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.6822 - accuracy: 0.3950 - val_loss: 1.6314 - val_accuracy: 0.4122\n",
            "Epoch 3/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.5927 - accuracy: 0.4262 - val_loss: 1.6669 - val_accuracy: 0.4037\n",
            "Epoch 4/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.5425 - accuracy: 0.4494 - val_loss: 1.5091 - val_accuracy: 0.4587\n",
            "Epoch 5/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.4997 - accuracy: 0.4614 - val_loss: 1.5515 - val_accuracy: 0.4479\n",
            "Epoch 6/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.4670 - accuracy: 0.4744 - val_loss: 1.4639 - val_accuracy: 0.4756\n",
            "Epoch 7/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.4392 - accuracy: 0.4840 - val_loss: 1.4782 - val_accuracy: 0.4763\n",
            "Epoch 8/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.4183 - accuracy: 0.4904 - val_loss: 1.4734 - val_accuracy: 0.4749\n",
            "Epoch 9/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.3889 - accuracy: 0.5036 - val_loss: 1.4454 - val_accuracy: 0.4790\n",
            "Epoch 10/300\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.3699 - accuracy: 0.5079 - val_loss: 1.4344 - val_accuracy: 0.4867\n",
            "Epoch 11/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.3493 - accuracy: 0.5164 - val_loss: 1.4693 - val_accuracy: 0.4739\n",
            "Epoch 12/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.3318 - accuracy: 0.5212 - val_loss: 1.4353 - val_accuracy: 0.4875\n",
            "Epoch 13/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.3140 - accuracy: 0.5311 - val_loss: 1.4317 - val_accuracy: 0.4988\n",
            "Epoch 14/300\n",
            "1563/1563 [==============================] - 43s 28ms/step - loss: 1.2947 - accuracy: 0.5362 - val_loss: 1.4490 - val_accuracy: 0.4884\n",
            "Epoch 15/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.2739 - accuracy: 0.5428 - val_loss: 1.4627 - val_accuracy: 0.4828\n",
            "Epoch 16/300\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.2646 - accuracy: 0.5468 - val_loss: 1.4501 - val_accuracy: 0.4922\n",
            "Epoch 17/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.2510 - accuracy: 0.5491 - val_loss: 1.4325 - val_accuracy: 0.5022\n",
            "Epoch 18/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.2338 - accuracy: 0.5563 - val_loss: 1.4245 - val_accuracy: 0.5039\n",
            "Epoch 19/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.2212 - accuracy: 0.5602 - val_loss: 1.5126 - val_accuracy: 0.4762\n",
            "Epoch 20/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.2059 - accuracy: 0.5648 - val_loss: 1.4620 - val_accuracy: 0.4965\n",
            "Epoch 21/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.1979 - accuracy: 0.5688 - val_loss: 1.4822 - val_accuracy: 0.4886\n",
            "Epoch 22/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.1778 - accuracy: 0.5770 - val_loss: 1.4557 - val_accuracy: 0.4985\n",
            "Epoch 23/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.1651 - accuracy: 0.5791 - val_loss: 1.4753 - val_accuracy: 0.4952\n",
            "Epoch 24/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.1568 - accuracy: 0.5833 - val_loss: 1.4736 - val_accuracy: 0.5046\n",
            "Epoch 25/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.1411 - accuracy: 0.5890 - val_loss: 1.4806 - val_accuracy: 0.5014\n",
            "Epoch 26/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.1306 - accuracy: 0.5925 - val_loss: 1.5047 - val_accuracy: 0.4955\n",
            "Epoch 27/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.1210 - accuracy: 0.5972 - val_loss: 1.4777 - val_accuracy: 0.5061\n",
            "Epoch 28/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.1081 - accuracy: 0.6004 - val_loss: 1.5405 - val_accuracy: 0.4882\n",
            "Epoch 29/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.1021 - accuracy: 0.6026 - val_loss: 1.5201 - val_accuracy: 0.5006\n",
            "Epoch 30/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0889 - accuracy: 0.6075 - val_loss: 1.5041 - val_accuracy: 0.5045\n",
            "Epoch 31/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.0797 - accuracy: 0.6113 - val_loss: 1.5515 - val_accuracy: 0.4918\n",
            "Epoch 32/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.0706 - accuracy: 0.6147 - val_loss: 1.5405 - val_accuracy: 0.4995\n",
            "Epoch 33/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0618 - accuracy: 0.6152 - val_loss: 1.5536 - val_accuracy: 0.4954\n",
            "Epoch 34/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.0527 - accuracy: 0.6187 - val_loss: 1.5307 - val_accuracy: 0.5045\n",
            "Epoch 35/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.0399 - accuracy: 0.6250 - val_loss: 1.5166 - val_accuracy: 0.5064\n",
            "Epoch 36/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0351 - accuracy: 0.6254 - val_loss: 1.5853 - val_accuracy: 0.4919\n",
            "Epoch 37/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.0304 - accuracy: 0.6280 - val_loss: 1.6017 - val_accuracy: 0.4897\n",
            "Epoch 38/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.0195 - accuracy: 0.6323 - val_loss: 1.6403 - val_accuracy: 0.4922\n",
            "Epoch 39/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0120 - accuracy: 0.6359 - val_loss: 1.6204 - val_accuracy: 0.4935\n",
            "Epoch 40/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9999 - accuracy: 0.6364 - val_loss: 1.6273 - val_accuracy: 0.4873\n",
            "Epoch 41/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9900 - accuracy: 0.6423 - val_loss: 1.6366 - val_accuracy: 0.4953\n",
            "Epoch 42/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.9860 - accuracy: 0.6433 - val_loss: 1.6791 - val_accuracy: 0.4827\n",
            "Epoch 43/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9785 - accuracy: 0.6471 - val_loss: 1.6420 - val_accuracy: 0.4889\n",
            "Epoch 44/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9697 - accuracy: 0.6485 - val_loss: 1.6915 - val_accuracy: 0.4892\n",
            "Epoch 45/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.9704 - accuracy: 0.6507 - val_loss: 1.7093 - val_accuracy: 0.4852\n",
            "Epoch 46/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.9656 - accuracy: 0.6522 - val_loss: 1.6562 - val_accuracy: 0.4967\n",
            "Epoch 47/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.9513 - accuracy: 0.6554 - val_loss: 1.7178 - val_accuracy: 0.4884\n",
            "Epoch 48/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9429 - accuracy: 0.6578 - val_loss: 1.7109 - val_accuracy: 0.4964\n",
            "Epoch 49/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.9451 - accuracy: 0.6582 - val_loss: 1.7586 - val_accuracy: 0.4838\n",
            "Epoch 50/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9332 - accuracy: 0.6631 - val_loss: 1.7416 - val_accuracy: 0.4765\n",
            "Epoch 51/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9315 - accuracy: 0.6628 - val_loss: 1.7497 - val_accuracy: 0.4840\n",
            "Epoch 52/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.9235 - accuracy: 0.6654 - val_loss: 1.7929 - val_accuracy: 0.4795\n",
            "Epoch 53/300\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.9204 - accuracy: 0.6657 - val_loss: 1.7463 - val_accuracy: 0.4896\n",
            "Epoch 54/300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.9136 - accuracy: 0.6670 - val_loss: 1.8055 - val_accuracy: 0.4830\n",
            "Epoch 55/300\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.9057 - accuracy: 0.6744 - val_loss: 1.8034 - val_accuracy: 0.4814\n",
            "Epoch 56/300\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.9004 - accuracy: 0.6736 - val_loss: 1.8322 - val_accuracy: 0.4800\n",
            "Epoch 57/300\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8939 - accuracy: 0.6786 - val_loss: 1.8496 - val_accuracy: 0.4840\n",
            "Epoch 58/300\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.8877 - accuracy: 0.6782 - val_loss: 1.8264 - val_accuracy: 0.4902\n",
            "Epoch 59/300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8800 - accuracy: 0.6798 - val_loss: 1.8623 - val_accuracy: 0.4842\n",
            "Epoch 60/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.8837 - accuracy: 0.6808 - val_loss: 1.8742 - val_accuracy: 0.4784\n",
            "Epoch 61/300\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8685 - accuracy: 0.6831 - val_loss: 1.8814 - val_accuracy: 0.4843\n",
            "Epoch 62/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.8690 - accuracy: 0.6857 - val_loss: 1.8826 - val_accuracy: 0.4823\n",
            "Epoch 63/300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8642 - accuracy: 0.6865 - val_loss: 1.9161 - val_accuracy: 0.4843\n",
            "Epoch 64/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.8592 - accuracy: 0.6889 - val_loss: 1.9551 - val_accuracy: 0.4748\n",
            "Epoch 65/300\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8554 - accuracy: 0.6894 - val_loss: 1.8912 - val_accuracy: 0.4811\n",
            "Epoch 66/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.8501 - accuracy: 0.6924 - val_loss: 1.9218 - val_accuracy: 0.4795\n",
            "Epoch 67/300\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8451 - accuracy: 0.6954 - val_loss: 1.9243 - val_accuracy: 0.4853\n",
            "Epoch 68/300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8446 - accuracy: 0.6942 - val_loss: 2.0082 - val_accuracy: 0.4773\n",
            "Epoch 69/300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8400 - accuracy: 0.6968 - val_loss: 1.9112 - val_accuracy: 0.4791\n",
            "Epoch 70/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.8299 - accuracy: 0.7010 - val_loss: 2.0362 - val_accuracy: 0.4833\n",
            "Epoch 71/300\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.8278 - accuracy: 0.7005 - val_loss: 2.0123 - val_accuracy: 0.4787\n",
            "Epoch 72/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.8220 - accuracy: 0.7009 - val_loss: 2.0203 - val_accuracy: 0.4743\n",
            "Epoch 73/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.8201 - accuracy: 0.7045 - val_loss: 2.0841 - val_accuracy: 0.4767\n",
            "Epoch 74/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.8141 - accuracy: 0.7029 - val_loss: 1.9983 - val_accuracy: 0.4856\n",
            "Epoch 75/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.8129 - accuracy: 0.7076 - val_loss: 2.0347 - val_accuracy: 0.4826\n",
            "Epoch 76/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.8077 - accuracy: 0.7088 - val_loss: 2.0729 - val_accuracy: 0.4742\n",
            "Epoch 77/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.8128 - accuracy: 0.7063 - val_loss: 2.0744 - val_accuracy: 0.4789\n",
            "Epoch 78/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.7899 - accuracy: 0.7140 - val_loss: 2.1303 - val_accuracy: 0.4729\n",
            "Epoch 79/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.7946 - accuracy: 0.7134 - val_loss: 2.0317 - val_accuracy: 0.4876\n",
            "Epoch 80/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.7914 - accuracy: 0.7120 - val_loss: 2.0750 - val_accuracy: 0.4760\n",
            "Epoch 81/300\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.7865 - accuracy: 0.7169 - val_loss: 2.0878 - val_accuracy: 0.4789\n",
            "Epoch 82/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.7937 - accuracy: 0.7131 - val_loss: 2.1727 - val_accuracy: 0.4774\n",
            "Epoch 83/300\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.7784 - accuracy: 0.7183 - val_loss: 2.1760 - val_accuracy: 0.4658\n",
            "Epoch 84/300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.7754 - accuracy: 0.7198 - val_loss: 2.2447 - val_accuracy: 0.4780\n",
            "Epoch 85/300\n",
            " 305/1563 [====>.........................] - ETA: 32s - loss: 0.7506 - accuracy: 0.7244"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtlNLPpXy/vQCHS51sW8kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anpham1331/ECGR4105-Intro-to-ML/blob/main/ECGR_4105_Homework_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRg3s3LgoLG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfa6e4b-ad35-45a6-957b-1935627ab562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/anpham1331/ECGR4105-Intro-to-ML/tree/main\n",
        "\n",
        "#uses Housing.csv for this assignment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_file2 = \"/content/drive/MyDrive/ECGR 4105 - Intro to ML/Datasets/Housing.csv\"\n",
        "housing = pd.read_csv(path_file2)\n",
        "\n",
        "# Defining the map function for housing. Maps yes to 1 and no to 0\n",
        "varlist = ['mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea']\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0})\n",
        "# Applying the function to the housing list\n",
        "housing[varlist] = housing[varlist].apply(binary_map)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1 - a\n",
        "#input variables = all input features\n",
        "\n",
        "# Get Values\n",
        "X = housing.iloc[:, 1:11].values\n",
        "y = housing.iloc[:, 0].values\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Scaling\n",
        "sc = StandardScaler()  # Create a scaler object\n",
        "X_train = sc.fit_transform(X_train)  # Fit the scaler to the training data and transform\n",
        "X_test = sc.transform(X_test)  # Apply the scaler to the test data\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "X_val = torch.FloatTensor(X_test)\n",
        "y_val = torch.FloatTensor(y_test)\n",
        "\n",
        "# Define the Neural Network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_size = X_train.shape[1]\n",
        "        self.hidden_size = 32\n",
        "        self.output_size = 1\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression problems\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#Model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    # Calculate average training loss\n",
        "    average_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validate the model every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss}, Validation Loss: {val_loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzA2NnE-HeTX",
        "outputId": "852539eb-72b8-410a-fbcf-9ffa7ebb37ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Training Loss: 26405848856283.43, Validation Loss: 25186000699392.0\n",
            "Epoch 200/1000, Training Loss: 26888380163218.285, Validation Loss: 25177052151808.0\n",
            "Epoch 300/1000, Training Loss: 26436019683328.0, Validation Loss: 25163953340416.0\n",
            "Epoch 400/1000, Training Loss: 26509164749970.285, Validation Loss: 25147171930112.0\n",
            "Epoch 500/1000, Training Loss: 26684062433280.0, Validation Loss: 25126986842112.0\n",
            "Epoch 600/1000, Training Loss: 26314034493147.43, Validation Loss: 25103551168512.0\n",
            "Epoch 700/1000, Training Loss: 26265025848466.285, Validation Loss: 25076801994752.0\n",
            "Epoch 800/1000, Training Loss: 26177359052800.0, Validation Loss: 25046846275584.0\n",
            "Epoch 900/1000, Training Loss: 26095339887469.715, Validation Loss: 25013719662592.0\n",
            "Epoch 1000/1000, Training Loss: 26426501009993.145, Validation Loss: 24977451515904.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Problem 1 - b\n",
        "#input variables = all input features\n",
        "\n",
        "# Get Values\n",
        "X = housing.iloc[:, 1:11].values\n",
        "y = housing.iloc[:, 0].values\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Scaling\n",
        "sc = StandardScaler()  # Create a scaler object\n",
        "X_train = sc.fit_transform(X_train)  # Fit the scaler to the training data and transform\n",
        "X_test = sc.transform(X_test)  # Apply the scaler to the test data\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "X_val = torch.FloatTensor(X_test)\n",
        "y_val = torch.FloatTensor(y_test)\n",
        "\n",
        "# Define the Neural Network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_size = X_train.shape[1]\n",
        "        self.hidden_size1 = 32\n",
        "        self.hidden_size2 = 64\n",
        "        self.hidden_size3 = 16\n",
        "        self.output_size = 1\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(self.hidden_size2, self.hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(self.hidden_size3, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression problems\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#Model\n",
        "train_loss_values = []\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    average_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validate the model every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss}, Validation Loss: {val_loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ak2jmWTGjP",
        "outputId": "d4adfd01-47a0-44fa-ab6a-1aca85eaf548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Training Loss: 9766406979584.0, Validation Loss: 9588175798272.0\n",
            "Epoch 200/1000, Training Loss: 2855308792393.143, Validation Loss: 2887528218624.0\n",
            "Epoch 300/1000, Training Loss: 1529168297984.0, Validation Loss: 1393320656896.0\n",
            "Epoch 400/1000, Training Loss: 1330609178916.5715, Validation Loss: 1204000391168.0\n",
            "Epoch 500/1000, Training Loss: 1226053713920.0, Validation Loss: 1124214112256.0\n",
            "Epoch 600/1000, Training Loss: 1189496918601.1428, Validation Loss: 1099398971392.0\n",
            "Epoch 700/1000, Training Loss: 1168788811190.8572, Validation Loss: 1081899417600.0\n",
            "Epoch 800/1000, Training Loss: 1185441068763.4285, Validation Loss: 1075194363904.0\n",
            "Epoch 900/1000, Training Loss: 1145984979529.1428, Validation Loss: 1072084549632.0\n",
            "Epoch 1000/1000, Training Loss: 1136873139638.8572, Validation Loss: 1067640029184.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2 - a\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n",
        "\n",
        "# Define the neural network model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(32, 32, 3)))  # Flatten the input images\n",
        "model.add(layers.Dense(512, activation='relu'))  # Hidden layer with 512 neurons and ReLU activation\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = model.evaluate(x_test, y_test)\n",
        "training_loss = history.history['loss'][-1]\n",
        "evaluation_accuracy = eval_result[1]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Training Loss: {training_loss:.4f}\")\n",
        "print(f\"Evaluation Accuracy: {evaluation_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSuM033bPU-g",
        "outputId": "aa6124bf-fdbf-4955-cb75-8d8e9e2be96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 44s 27ms/step - loss: 1.8909 - accuracy: 0.3298 - val_loss: 1.7388 - val_accuracy: 0.3811\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.7044 - accuracy: 0.3891 - val_loss: 1.6268 - val_accuracy: 0.4155\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.6307 - accuracy: 0.4173 - val_loss: 1.5968 - val_accuracy: 0.4299\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5872 - accuracy: 0.4324 - val_loss: 1.5996 - val_accuracy: 0.4252\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.5562 - accuracy: 0.4442 - val_loss: 1.5503 - val_accuracy: 0.4464\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5369 - accuracy: 0.4526 - val_loss: 1.5835 - val_accuracy: 0.4358\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5216 - accuracy: 0.4573 - val_loss: 1.5734 - val_accuracy: 0.4406\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.5040 - accuracy: 0.4626 - val_loss: 1.5317 - val_accuracy: 0.4462\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.4877 - accuracy: 0.4715 - val_loss: 1.5294 - val_accuracy: 0.4566\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 1.4761 - accuracy: 0.4743 - val_loss: 1.5613 - val_accuracy: 0.4418\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 1.5613 - accuracy: 0.4418\n",
            "Training Time: 384.16 seconds\n",
            "Training Loss: 1.4761\n",
            "Evaluation Accuracy: 0.4418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 2 - b\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n",
        "\n",
        "# Define the extended neural network model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(32, 32, 3)))  # Flatten the input images\n",
        "model.add(layers.Dense(512, activation='relu'))  # Hidden layer 1 with 512 neurons and ReLU activation\n",
        "model.add(layers.Dense(256, activation='relu'))  # Hidden layer 2 with 256 neurons and ReLU activation\n",
        "model.add(layers.Dense(128, activation='relu'))  # Hidden layer 3 with 128 neurons and ReLU activation\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=300, validation_data=(x_test, y_test))\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = model.evaluate(x_test, y_test)\n",
        "training_loss = history.history['loss'][-1]\n",
        "evaluation_accuracy = eval_result[1]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Training Loss: {training_loss:.4f}\")\n",
        "print(f\"Evaluation Accuracy: {evaluation_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "_wwzOrXQRloy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}